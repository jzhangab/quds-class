{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to Applied Machine Learning using a Real Business Case\n",
    "## Operations Example\n",
    "In this notebook you will implement a machine learning algorithm to model a complex system that has been anonymized from a real problem within the Operations space. You will understand how to look beyond single factor analysis and utilize machine learning to model the system and also discover key factors that contribute to failure modes.\n",
    "\n",
    "The data has been anonymized, randomized, and any identifying information removed. However, it still is still heavily derived from operations data you may have seen before and is a great way to begin thinking about how you can use multifactor analysis and machine learning to solve problems in your space\n",
    "\n",
    "## Contents\n",
    "1. Import dataset\n",
    "2. Data exploration\n",
    "3. Feature engineering\n",
    "4. Modeling\n",
    "5. Model Evaluation\n",
    "\n",
    "## How to use this notebook\n",
    "- To execute any single block of text or markdown, use ctrl+enter, shift+enter or press the run arrow on the left of the box (only in Colaboratory)\n",
    "- To reset the notebook select \"Factory reset runtime\" from the Runtime tab at the top of Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import our data. The data is split into 2 files\n",
    "# data_url contains manufacturing-related data that describes the conditions that the product was manufactured\n",
    "# label_url contains information related to the outcome of each product subset - and we will discuss later what constitutes bad vs. good product\n",
    "import pandas as pd\n",
    "\n",
    "data_url = 'https://raw.githubusercontent.com/jzhangab/DS101/master/1_Data/data.csv'\n",
    "label_url = 'https://raw.githubusercontent.com/jzhangab/DS101/master/1_Data/outcome.csv'\n",
    "df = pd.read_csv(data_url, sep = ',')\n",
    "df_label = pd.read_csv(label_url, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 5 rows to begin understanding what factors are available\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the outcome\n",
    "df_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can begin thinking/answering some questions you may have about this data.\n",
    "\n",
    "1. How is each data point separated? What denotes a unique data point?\n",
    "2. How many data points do we have?\n",
    "3. Do we have any missing data?\n",
    "4. Do we only have numerical data?\n",
    "5. How many features do we potentially have?\n",
    "6. What is the question we are trying to answer with this data, can this data answer this question?\n",
    "7. etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Exploration\n",
    "Let's now begin exploring the data we have to help ourselves understand the problem and see what approach we might want to take.\n",
    "\n",
    "Before we can begin plotting our data, we have to deal with 2 problems that we encounter in this dataset that we did not encounter in the Pima Indian Diabetes data set.\n",
    "\n",
    "1. There is a value \"Low Value\" that is a replacement for undetectable signal. We have to replace this with a number.\n",
    "2. There are null values in our parameter columns that we need to fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleanliness, missing data is very important, let's check how much missing data there is for each factor\n",
    "for col in list(df):\n",
    "    num_na = len(df[col]) - df[col].count()\n",
    "    print (\"Percent null in column \" + col + \" is:\", 100*num_na/len(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's check the number of \"Low Value\" in each column\n",
    "for col in list(df):\n",
    "    num_lv = len(df.loc[df[col] == 'Low Value'][col])\n",
    "    print (\"Percent Low Value in column \" + col + \" is:\", 100*num_lv/len(df[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observations\n",
    "1. Thanks to a-priori domain knowledge, we know that \"Low Value\" is a placeholder for any measurement that is less than 0.5. Therefore we will replace this by the average of values 0-0.5 (0.25)\n",
    "2. A-priori we don't care about the DATE of manufacture because we know that our process should be in control regardless of the date so we will not consider it for this analysis. This does not mean that in your own analysis on a separate problem that DATE will not be important.\n",
    "3. We also do not care for the DESCRIPTOR_1 and SOURCE columns because we know they are unrelated to manufacturing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the following to clean our data\n",
    "# 1. Replace all null values with 0\n",
    "# 2. Replace Low Value with 0.25\n",
    "# 3. Convert parameter columns to float64 from string objects, we have to do this because \"Low Value\" defaults each column to string objects\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df = df.replace(\"Low Value\", 0.25)\n",
    "\n",
    "for c in list(df):\n",
    "    if \"PARAM\" in c:\n",
    "        df[c] = df[c].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's remove the non-parametric columns from our dataframe\n",
    "df = df[[c for c in list(df) if c not in ['SOURCE', 'DATE', 'DESCRIPTOR_1']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observation\n",
    "Another potential issue for us is determining what to do with duplicate data. We know a-priori that each data point should be unique by the NAME column, but how do we handle this if each NAME has multiple sub-names? We need to do the following.\n",
    "\n",
    "1. Figure out if there are duplicate entires by the NAME column\n",
    "2. If there are, we need to average all data by SUB_NAME for each NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there duplicate names? We can check for this by doing a value count for the NAME column\n",
    "df['NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like there are some NAMEs with up to 32 SUB-NAMEs, we need to average the PARAM values for each NAME\n",
    "df = df.groupby(['NAME']).mean()\n",
    "\n",
    "# Reset the indices and then remove SUB_NAME from columns because we no longer care about the SUB_NAMEs after averaging by NAME\n",
    "df = df.reset_index()\n",
    "df = df[[c for c in list(df) if c != 'SUB_NAME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the histograms of the dataframe to understand each factor.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "ax = fig.gca()\n",
    "df.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This is a very interesting dataset! There are some key observations that we can make.\n",
    "\n",
    "1. Some of the data is normally distributed (PARAM_1, PARAM_10, PARAM_2, PARAM_4)\n",
    "2. Some of the data is skewed (PARAM_3, PARAM_7, PARAM_8)\n",
    "3. Some of the data is bimodal (PARAM_5, PARAM_6, PARAM_9)\n",
    "4. Some of the parameters are between 0 and 1 while others are between 0 and 10, is this going to be a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The non-normally distributed data is very interesting, perhaps they are related to each other?\n",
    "%matplotlib inline\n",
    "df.plot.scatter(x = 'PARAM_5',\n",
    "                y = 'PARAM_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using what you have learned about plots, try plotting some of the columns using histograms, scatterplots, or regression models to explore the data\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Feature Engineering\n",
    "Since we have already cleaned our data, the purpose of feature engineering in this exercise is to determine which columns we will use as features and also merge the labels (outcomes)\n",
    "\n",
    "1. Labeling\n",
    "2. Merge data\n",
    "3. Decide which features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels are located in the dataframe df_label\n",
    "# df_label has the same issue as the parameter dataframe in that there are NAMEs with duplicates by SUB_NAME\n",
    "# Let's aggregate by average on NAME and remove SUB_NAME\n",
    "df_label['NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df_label.groupby(['NAME']).mean()\n",
    "\n",
    "# Reset the indices and then remove SUB_NAME from columns because we no longer care about the SUB_NAMEs after averaging by NAME\n",
    "df_label = df_label.reset_index()\n",
    "df_label = df_label[[c for c in list(df_label) if c != 'SUB_NAME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observation\n",
    "How do we determine if a particular data point represents bad product? Well thanks to a-priori knowledge we know that there is an existing process-monitoring and controls action limit at VALUE = 2.0. Let's explore the label using this action limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's plot a histogram to visualize the label measure distribution\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "ax = fig.gca()\n",
    "df_label.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm, by the way how many standard deviations from the mean is 2.0?\n",
    "sd = df_label['MEASURE'].std()\n",
    "mean = df_label['MEASURE'].mean()\n",
    "\n",
    "print(\"Action limit num SD from mean:\", (2-mean)/sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observation\n",
    "Very interesting! There are some key observations we can make.\n",
    "\n",
    "1. The label measure itself appears to be normally distributed.\n",
    "2. The action limit is 2.85 SD from mean which means our system does not quite meet the 3.0 SD threshold normally found in most preventive capability analysis systems.\n",
    "3. We have an imbalanced dataset, there are far more \"good\" datapoints than \"bad\" datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a classification label, we need to convert the continuous MEASURE into binary (0 or 1) LABEL\n",
    "df_label['LABEL'] = 0\n",
    "df_label.loc[df_label['MEASURE'] >= 2, 'LABEL'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multicollinearity\n",
    "The idea of collinearity is that if certain input factors are closely correlated, they will bias the output of the model by amplifying their particular effects. We need to understand if some of our factors are high collinear and then reduce bias by removing all but 1 of the collinear factors from the dataframe.\n",
    "\n",
    "1. Let's check if we have a collinearity problem in our parameter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the correlation (R-square) between variables using a correlation matrix\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To quantify multicollinearity, we will use variance inflation factor (VIF)\n",
    "# Rule of thumb, VIF above 10 indicates a particular variable ought to be removed\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Also - VIF for a constant term should be high because the intercept is a proxy for the constant.\n",
    "df_c = add_constant(df[[c for c in list(df) if 'PARAM' in c]])\n",
    "pd.Series([variance_inflation_factor(df_c.values, i) \n",
    "               for i in range(df_c.shape[1])], \n",
    "              index=df_c.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we have to merge the feature and label dataframes for the modeling phase of the analysis\n",
    "df = pd.merge(df, df_label, on = 'NAME', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Modeling\n",
    "In the modeling step we will train a supervised machine learning model to understand relationships in the diabetes data set. We will then evaluate the model to see how well it predicts.\n",
    "\n",
    "The particular model that we will use is the random forest algorithm, a classical machine learning algorithm very useful for classification tasks.\n",
    "\n",
    "1. Split dataset into training and validation datasets\n",
    "2. Train model\n",
    "3. Predict outcomes of validation dataset\n",
    "4. Calculate accuracy of validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split the data 80%/20% using 80% of the data to train the model and 20% to validate the accuracy of the model\n",
    "# We can use pre-built functions from the machine learning package sci-kit learn to do this task\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "features = [col for col in list(df) if col not in ['NAME', 'MEASURE', 'LABEL']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df['LABEL'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and fit model\n",
    "# We have to use the hyperparameter class_weight because of the very imbalanced classes (very few 1's compared to 0's)\n",
    "# Balanced class weight uses the inverse of frequency to weight each class\n",
    "model = RandomForestClassifier(random_state=0, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Model Evaluation\n",
    "We will use several techniques to evaluate the strength of the Model\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix (false positive, true positive, false negative, true negative)\n",
    "3. Receiver operating characteristic\n",
    "4. Sigmoid probability visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test (true values) to y_pred (predicted values)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the confusion matrix, which shows us false positives and false negatives\n",
    "# According to the confusion matrix we have only a single false negative and selected correctly the other 2 positive classes\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method of evaluating a classifier is using the Receiver Operating Characteristic (ROC)\n",
    "# ROC is a plot of true positive vs. false positive. We calculate the area under the curve (AUC)\n",
    "# AUC = 1 indicates a perfect classifier, AUC = 0.5 means the classifier is no better than a coin flip\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "falseposrate, trueposrate, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(falseposrate,trueposrate,label=\"ROC curve, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# With tree models we can use feature importance to get some insight into root cause\n",
    "\n",
    "# Mean feature importance across all trees\n",
    "mean_i = model.feature_importances_\n",
    "\n",
    "# Standard deviation of feature importances across all trees\n",
    "st_i = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "# Features\n",
    "features_i = np.argsort(mean_i)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. PARAM %d (%f)\" % (f + 1, features_i[f]+1, mean_i[features_i[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observation\n",
    "It appears we can make some preliminary analysis about the root cause of the products that fall above the action limit. Next steps for such an analysis would be to investigate the physical relationship between PARAM_8 and the manufacturing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}